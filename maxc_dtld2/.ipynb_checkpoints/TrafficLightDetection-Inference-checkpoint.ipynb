{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Light Detection and Classification\n",
    "Using a pre-trained model to detect objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensorflow 1.1x\n",
    "# cd to maxc\n",
    "# TODO\n",
    "# 1. imports from the object detection module \n",
    "# 2. Model preparation (paths setup)\n",
    "# 3. jpg or png\n",
    "# 4. vedio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Salles\\TensorFlow\\models\\research\\object_detection\\utils\\visualization_utils_bstld.py:29: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 759, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-b5d7f522861e>\", line 12, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<C:\\ProgramData\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-108>\", line 2, in matplotlib\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# imports from the object detection module TODO\n",
    "# sys.path.append('C:\\\\Salles\\\\TensorFlow\\\\models\\\\models\\\\research\\\\object_detection')\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils_bstld as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names =['ssd_mobilenet_v2','ssd_inception_v2','faster_rcnn_inception_v2','faster_rcnn_resnet101','rfcn_resnet101'] \n",
    "model_name = model_names[2]\n",
    "# model_name = model_names[1]+'_lr' # TODO\n",
    "model_path = 'trained_inference_graphs/output_inference_graph_%s/frozen_inference_graph.pb' % model_name\n",
    "PATH_TO_LABELS = 'annotations/dtld_state_label_map2.pbtxt'\n",
    "\n",
    "dataset_dir = 'maxc_dtld2'\n",
    "PATH_TO_FOLDER = 'videos'\n",
    "\n",
    "path_names=['Berlin', 'Bochum', 'Bremen', 'Dortmund', 'Duesseldorf', 'Essen', 'Frankfurt', 'Fulda', 'Hannover', 'Kassel', 'Koeln']\n",
    "\n",
    "PATH_TO_IMAGES_DIR = 'videos\\\\'+path_names[0]\n",
    "\n",
    "EVAL_IMAGES_DIR = PATH_TO_IMAGES_DIR + '\\\\output\\\\%s' % model_name\n",
    "\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map\n",
    "\n",
    "Label maps map indices to category names, so that when our convolution network predicts 2, we know that this corresponds to Red. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': 'Red-yellow'}, 2: {'id': 2, 'name': 'Green'}, 3: {'id': 3, 'name': 'Yellow'}, 4: {'id': 4, 'name': 'Red'}}\n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "\n",
    "  with tf.gfile.GFile(model_path, 'rb') as fid:        \n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Salles\\TensorFlow\\workspace\\maxc_dtld2\\videos\\Berlin\\*.jpg\n",
      "Length of test image paths: 20\n"
     ]
    }
   ],
   "source": [
    "abs_paths=os.path.join(os.getcwd(),PATH_TO_IMAGES_DIR, '*.jpg')\n",
    "print(abs_paths)\n",
    "TEST_IMAGE_PATHS = glob(abs_paths)\n",
    "TEST_IMAGE_PATHS = sorted(TEST_IMAGE_PATHS)\n",
    "print(\"Length of test image paths:\", len(TEST_IMAGE_PATHS))\n",
    "\n",
    "# test_idxs = [890, 961, 3738]\n",
    "# TEST_IMAGE_PATHS = ['%s/frame_%06d.jpg'% (PATH_TO_IMAGES_DIR, idx) for idx in test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_plot_result = False\n",
    "is_make_video = True\n",
    "min_score_thresh = .50 # TODO\n",
    "\n",
    "def draw_a_detection_result(boxes, scores, classes, num, image_np, t_elapsed):\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes).astype(np.int32)\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np, boxes, classes, scores,\n",
    "        category_index,\n",
    "        min_score_thresh=min_score_thresh,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=3)\n",
    "\n",
    "    if is_plot_result:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(image_np)\n",
    "        plt.show()\n",
    "\n",
    "#         print 'num boxes =', boxes.shape[0]\n",
    "#         print num, classes, scores\n",
    "#         print image_np.shape\n",
    "\n",
    "        for i in range(boxes.shape[0]):\n",
    "            if scores is None or scores[i] > min_score_thresh:\n",
    "                class_name = category_index[classes[i]]['name']\n",
    "                print('{}'.format(class_name), scores[i])                  \n",
    "                #print 'box:', boxes[i] # ymin, xmin, ymax, xmax = box\n",
    "                print(\"Time in milliseconds\", t_elapsed * 1000, \"\\n\")\n",
    "\n",
    "                \n",
    "def make_video(images, outvid='output.avi', fps=5, size=None,\n",
    "               is_color=True, format=\"XVID\"):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      images      list of images to use in the video\n",
    "    @param      outvid      output video name\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    " \n",
    "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
    "    By default, the video will have the size of the first image.\n",
    "    It will resize every image to this size before adding them to the video.\n",
    "    \n",
    "    from: http://www.xavierdupre.fr/blog/2016-03-30_nojs.html\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time for each picture:\n",
      "5.505640268325806\n",
      "0.07294297218322754\n",
      "0.07793831825256348\n",
      "0.08093595504760742\n",
      "0.07793807983398438\n",
      "0.08093619346618652\n",
      "0.07947921752929688\n",
      "0.07794046401977539\n",
      "0.07893848419189453\n",
      "0.07794070243835449\n",
      "0.07793903350830078\n",
      "0.07893848419189453\n",
      "0.07969498634338379\n",
      "0.07893824577331543\n",
      "0.07893776893615723\n",
      "0.07993650436401367\n",
      "0.07893753051757812\n",
      "0.07793903350830078\n",
      "0.07993793487548828\n",
      "0.07993745803833008\n"
     ]
    }
   ],
   "source": [
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        \n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        \n",
    "        def eval_an_image(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            time0 = time.time()\n",
    "\n",
    "            # Actual detection.\n",
    "            ret = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            \n",
    "            time1 = time.time()\n",
    "            return ret, image_np, time1 - time0\n",
    "        \n",
    "        all_time_elapsed = []\n",
    "        print('inference time for each picture:')\n",
    "        for image_path in TEST_IMAGE_PATHS:\n",
    "            (boxes, scores, classes, num), image_np, time_elapsed = eval_an_image(image_path)\n",
    "            print(time_elapsed)\n",
    "            all_time_elapsed.append(time_elapsed)\n",
    "            \n",
    "            draw_a_detection_result(boxes, scores, classes, num, image_np, time_elapsed)                \n",
    "            if is_make_video:\n",
    "                if not os.path.exists(EVAL_IMAGES_DIR):\n",
    "                    os.makedirs(EVAL_IMAGES_DIR)\n",
    "                Image.fromarray(image_np).save(EVAL_IMAGES_DIR + '/' + os.path.basename(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: faster_rcnn_inception_v2\n",
      "tot_time_elapsed: 1.50 s\n",
      "avg_time_elapsed: 78.74 ms\n"
     ]
    }
   ],
   "source": [
    "print('model:', model_name)\n",
    "tot_time_elapsed = sum(all_time_elapsed[1:])\n",
    "num_images=len(all_time_elapsed)-1\n",
    "print ('tot_time_elapsed: %.2f s' %tot_time_elapsed)\n",
    "print ('avg_time_elapsed: %.2f ms' % (tot_time_elapsed/num_images*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inferred images: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<VideoWriter 000002783F9F1F10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Video\n",
    "eval_images = sorted(glob(os.path.join(EVAL_IMAGES_DIR, '*.jpg')))\n",
    "print(\"Length of inferred images:\", len(eval_images))\n",
    "make_video(eval_images, outvid=EVAL_IMAGES_DIR+'/inferred.avi', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
